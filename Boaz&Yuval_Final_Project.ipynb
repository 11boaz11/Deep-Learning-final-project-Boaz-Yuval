{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDzSYKBuUHgt"
      },
      "outputs": [],
      "source": [
        "# 1. ENVIRONMENT SETUP\n",
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "!apt-get install -y libopenmpi-dev\n",
        "!pip install mpi4py\n",
        "!pip install datasets einops\n",
        "!pip install torchmetrics torch-fidelity\n",
        "!pip install blobfile\n",
        "!git clone https://github.com/openai/guided-diffusion.git\n",
        "%cd guided-diffusion\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "\n",
        "import sys\n",
        "sys.path.append(os.path.abspath(\"./guided-diffusion\"))\n",
        "\n",
        "# 2. IMPORTS\n",
        "import copy\n",
        "import gc\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import urllib.request\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from datasets import load_dataset\n",
        "\n",
        "from guided_diffusion import script_util\n",
        "\n",
        "# 3. DEVICE CHECK\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup & Weights ---\n",
        "MODELS_DIR = \"./models\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "WEIGHTS_PATH = f\"{MODELS_DIR}/64x64_diffusion.pt\"\n",
        "\n",
        "if not os.path.exists(WEIGHTS_PATH):\n",
        "    print(\"Downloading pretrained weights...\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64x64_diffusion.pt\",\n",
        "            WEIGHTS_PATH\n",
        "        )\n",
        "        print(f\"Weights saved to: {WEIGHTS_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Download failed: {e}\")\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "@dataclass\n",
        "class Config:\n",
        "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    SEED: int = 123\n",
        "\n",
        "    RUN_DIR: str = \"./ddgan_run\"\n",
        "    CHECKPOINT_PATH: str = \"./ddgan_run/saved_checkpoint.pt\"\n",
        "    START_NEW_RUN: bool = True\n",
        "\n",
        "    TRAIN_OPTIM_STEPS_THIS_RUN: int = 1000\n",
        "    IMAGE_SIZE: int = 64\n",
        "    NUM_CLASSES: int = 1000\n",
        "    BATCH_SIZE: int = 32\n",
        "    LR_G: float = 5e-6\n",
        "    LR_D: float = 4e-5\n",
        "    WEIGHT_DECAY_G: float = 0.0\n",
        "    WEIGHT_DECAY_D: float = 0.0\n",
        "    GRAD_CLIP_NORM: float = 1.0\n",
        "    LOG_EVERY: int = 10\n",
        "    SAVE_EVERY: int = 100\n",
        "\n",
        "    TARGET_CLASSES: List[int] = None  # defaults to [207, 250]\n",
        "    MAX_SAMPLES_PER_CLASS: Optional[int] = 500\n",
        "    NUM_WORKERS: int = 0\n",
        "    SHUFFLE: bool = True\n",
        "\n",
        "    DIFFUSION_BASE_STEPS: int = 1000\n",
        "    DIFFUSION_RESPACING: str = \"100\"\n",
        "    USE_FP16: bool = False\n",
        "\n",
        "    LAMBDA_ADV: float = 0.002\n",
        "    ADV_GUIDANCE_REGION: float = 0.5\n",
        "    D_UPDATES_PER_G: int = 2\n",
        "\n",
        "    REC_ADV_RAMP_SCHEDULE: bool = True\n",
        "    ADV_RAMP_STEPS: int = 1000\n",
        "\n",
        "    EVAL_NUM_IMAGES: int = 4\n",
        "    EVAL_SAVE_PATH: str = \"./ddgan_run/eval_grid.png\"\n",
        "\n",
        "    REC_FREEZE_D_BACKBONE_BN_STATS: bool = True\n",
        "    REC_USE_P_MEAN_VARIANCE_PRED_XSTART: bool = True\n",
        "    PRETRAINED_DIFFUSION_WEIGHTS = WEIGHTS_PATH\n",
        "\n",
        "cfg = Config()\n",
        "if cfg.TARGET_CLASSES is None:\n",
        "    cfg.TARGET_CLASSES = [207, 250]  # Golden Retriever & Siberian Husky\n",
        "\n",
        "os.makedirs(cfg.RUN_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(cfg.RUN_DIR, \"models\"), exist_ok=True)\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(cfg.SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "print(f\"=== CONFIG ===\\nSteps: {cfg.TRAIN_OPTIM_STEPS_THIS_RUN} | Lambda: {cfg.LAMBDA_ADV} | Ramp: {cfg.ADV_RAMP_STEPS}\")\n",
        "\n",
        "\n",
        "# --- Dataset ---\n",
        "def aggressive_cleanup():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Loading Dataset...\")\n",
        "full_ds = load_dataset(\"benjamin-paine/imagenet-1k-64x64\", split=\"train\")\n",
        "target_set = set(cfg.TARGET_CLASSES)\n",
        "filtered_ds = full_ds.filter(lambda x: int(x[\"label\"]) in target_set)\n",
        "\n",
        "if cfg.MAX_SAMPLES_PER_CLASS is not None:\n",
        "    counts = {c: 0 for c in cfg.TARGET_CLASSES}\n",
        "    indices = []\n",
        "    for i in range(len(filtered_ds)):\n",
        "        lbl = int(filtered_ds[i][\"label\"])\n",
        "        if counts[lbl] < cfg.MAX_SAMPLES_PER_CLASS:\n",
        "            indices.append(i)\n",
        "            counts[lbl] += 1\n",
        "    filtered_ds = filtered_ds.select(indices)\n",
        "\n",
        "print(f\"Dataset Size: {len(filtered_ds)} images.\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, labels = [], []\n",
        "    for ex in batch:\n",
        "        x = torch.from_numpy(np.array(ex[\"image\"], dtype=np.uint8)).permute(2, 0, 1).float() / 255.0\n",
        "        x = x * 2.0 - 1.0  # normalize to [-1, 1]\n",
        "        imgs.append(x)\n",
        "        labels.append(int(ex[\"label\"]))\n",
        "    return torch.stack(imgs, dim=0), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "loader = DataLoader(filtered_ds, batch_size=cfg.BATCH_SIZE, shuffle=cfg.SHUFFLE, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# --- Models ---\n",
        "class ConditionalResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=1000, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.class_emb = nn.Embedding(num_classes, embed_dim)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512 + embed_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = (x + 1.0) / 2.0\n",
        "        x = (x - self.mean) / self.std\n",
        "        feats = self.backbone(x)\n",
        "        emb = self.class_emb(labels)\n",
        "        return self.head(torch.cat([feats, emb], dim=1)).squeeze(1)\n",
        "\n",
        "D = ConditionalResNet18(num_classes=1000).to(cfg.DEVICE).float()\n",
        "for p in D.backbone.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "diffusion_args = script_util.model_and_diffusion_defaults()\n",
        "diffusion_args.update({\n",
        "    \"image_size\": 64,\n",
        "    \"num_channels\": 192,\n",
        "    \"num_res_blocks\": 3,\n",
        "    \"learn_sigma\": True,\n",
        "    \"class_cond\": True,\n",
        "    \"use_checkpoint\": True,\n",
        "    \"attention_resolutions\": \"32,16,8\",\n",
        "    \"num_heads\": 4,\n",
        "    \"num_head_channels\": 64,\n",
        "    \"use_scale_shift_norm\": True,\n",
        "    \"dropout\": 0.1,\n",
        "    \"resblock_updown\": True,\n",
        "    \"use_fp16\": cfg.USE_FP16,\n",
        "    \"use_new_attention_order\": True,\n",
        "    \"diffusion_steps\": cfg.DIFFUSION_BASE_STEPS,\n",
        "    \"noise_schedule\": \"cosine\",\n",
        "    \"timestep_respacing\": cfg.DIFFUSION_RESPACING,\n",
        "    \"channel_mult\": \"1,2,3,4\",\n",
        "    \"num_heads_upsample\": -1,\n",
        "    \"use_kl\": False,\n",
        "    \"predict_xstart\": False,\n",
        "    \"rescale_timesteps\": True,\n",
        "    \"rescale_learned_sigmas\": False,\n",
        "})\n",
        "\n",
        "G, diffusion = script_util.create_model_and_diffusion(**diffusion_args)\n",
        "G.to(cfg.DEVICE).float()\n",
        "\n",
        "print(\"Loading pretrained weights...\")\n",
        "ckpt = torch.load(cfg.PRETRAINED_DIFFUSION_WEIGHTS, map_location=\"cpu\")\n",
        "\n",
        "def _extract_state_dict(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return obj.get(\"ema\") or obj.get(\"state_dict\") or obj.get(\"model\") or obj\n",
        "    return obj\n",
        "\n",
        "G.load_state_dict(_extract_state_dict(ckpt), strict=False)\n",
        "print(\"Weights loaded.\")\n",
        "\n",
        "\n",
        "# --- Optimizers & Checkpoints ---\n",
        "opt_G = torch.optim.AdamW(G.parameters(), lr=cfg.LR_G)\n",
        "opt_D = torch.optim.AdamW(filter(lambda p: p.requires_grad, D.parameters()), lr=cfg.LR_D)\n",
        "\n",
        "def save_checkpoint(step, G, D, opt_G, opt_D):\n",
        "    path = cfg.CHECKPOINT_PATH\n",
        "    ckpt = {\n",
        "        \"step\": step, \"G_state\": G.state_dict(), \"D_state\": D.state_dict(),\n",
        "        \"opt_G_state\": opt_G.state_dict(), \"opt_D_state\": opt_D.state_dict(), \"cfg\": asdict(cfg)\n",
        "    }\n",
        "    tmp_path = path + \".tmp\"\n",
        "    torch.save(ckpt, tmp_path)\n",
        "    if os.path.exists(path):\n",
        "        os.remove(path)\n",
        "    os.rename(tmp_path, path)\n",
        "\n",
        "global_step = 0\n",
        "if cfg.START_NEW_RUN and os.path.exists(cfg.CHECKPOINT_PATH):\n",
        "    print(\"Starting new run — deleting old checkpoint.\")\n",
        "    os.remove(cfg.CHECKPOINT_PATH)\n",
        "elif os.path.exists(cfg.CHECKPOINT_PATH):\n",
        "    try:\n",
        "        ckpt = torch.load(cfg.CHECKPOINT_PATH, map_location=cfg.DEVICE)\n",
        "        global_step = ckpt[\"step\"]\n",
        "        G.load_state_dict(ckpt[\"G_state\"])\n",
        "        D.load_state_dict(ckpt[\"D_state\"])\n",
        "        print(f\"Resumed from step {global_step}\")\n",
        "    except Exception:\n",
        "        print(\"Checkpoint corrupted — starting fresh.\")\n",
        "\n",
        "\n",
        "# --- Phase 1: Discriminator Warm-up ---\n",
        "print(\"Phase 1: Warming up Discriminator with frozen Generator...\")\n",
        "\n",
        "G.eval()\n",
        "for p in G.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "UNFREEZE_D_BACKBONE = False\n",
        "\n",
        "if UNFREEZE_D_BACKBONE:\n",
        "    for p in D.backbone.parameters():\n",
        "        p.requires_grad_(True)\n",
        "    opt_D_warmup = torch.optim.AdamW(D.parameters(), lr=cfg.LR_D)\n",
        "else:\n",
        "    print(\"   Keeping D backbone frozen...\")\n",
        "    opt_D_warmup = opt_D\n",
        "\n",
        "D.train()\n",
        "if cfg.REC_FREEZE_D_BACKBONE_BN_STATS:\n",
        "    D.backbone.eval()\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def get_pred_xstart_warmup(x_t, t, y):\n",
        "    out = diffusion.p_mean_variance(G, x_t, t, model_kwargs={\"y\": y})\n",
        "    return out[\"pred_xstart\"].clamp(-1, 1)\n",
        "\n",
        "WARMUP_STEPS = 500\n",
        "warmup_history = {'step': [], 'd_loss': [], 'd_real': [], 'd_fake': []}\n",
        "\n",
        "data_iter = iter(loader)\n",
        "progress = tqdm(total=WARMUP_STEPS, desc=\"D Warm-up\")\n",
        "\n",
        "for step in range(WARMUP_STEPS):\n",
        "    try:\n",
        "        real_x, y = next(data_iter)\n",
        "    except StopIteration:\n",
        "        data_iter = iter(loader)\n",
        "        real_x, y = next(data_iter)\n",
        "\n",
        "    real_x, y = real_x.to(cfg.DEVICE), y.to(cfg.DEVICE)\n",
        "    t = torch.randint(0, int(diffusion.num_timesteps), (real_x.shape[0],), device=cfg.DEVICE)\n",
        "    noise = torch.randn_like(real_x)\n",
        "    x_t = diffusion.q_sample(real_x, t, noise=noise)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake_x0 = get_pred_xstart_warmup(x_t, t, y)\n",
        "\n",
        "    real_logits = D(real_x, y)\n",
        "    fake_logits = D(fake_x0, y)\n",
        "\n",
        "    d_loss = bce(real_logits, torch.ones_like(real_logits)) + bce(fake_logits, torch.zeros_like(fake_logits))\n",
        "\n",
        "    opt_D_warmup.zero_grad()\n",
        "    d_loss.backward()\n",
        "    opt_D_warmup.step()\n",
        "\n",
        "    warmup_history['step'].append(step)\n",
        "    warmup_history['d_loss'].append(d_loss.item())\n",
        "    warmup_history['d_real'].append(torch.sigmoid(real_logits).mean().item())\n",
        "    warmup_history['d_fake'].append(torch.sigmoid(fake_logits).mean().item())\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        progress.set_description(\n",
        "            f\"D_loss={d_loss.item():.3f} | Real={warmup_history['d_real'][-1]:.2f} | Fake={warmup_history['d_fake'][-1]:.2f}\"\n",
        "        )\n",
        "    progress.update(1)\n",
        "\n",
        "progress.close()\n",
        "\n",
        "print(\"\\nPhase 1 Complete!\")\n",
        "print(f\"   Final D scores — Real: {warmup_history['d_real'][-1]:.3f}, Fake: {warmup_history['d_fake'][-1]:.3f}\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax1.plot(warmup_history['step'], warmup_history['d_loss'])\n",
        "ax1.set_title(\"D Loss during Warm-up\")\n",
        "ax1.set_xlabel(\"Step\"); ax1.set_ylabel(\"Loss\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(warmup_history['step'], warmup_history['d_real'], label='Real → D')\n",
        "ax2.plot(warmup_history['step'], warmup_history['d_fake'], label='Fake → D')\n",
        "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "ax2.set_title(\"D Predictions during Warm-up\")\n",
        "ax2.set_xlabel(\"Step\"); ax2.set_ylabel(\"Sigmoid(logit)\")\n",
        "ax2.legend(); ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "for p in G.parameters():\n",
        "    p.requires_grad_(True)\n",
        "G.train()\n",
        "\n",
        "if UNFREEZE_D_BACKBONE:\n",
        "    for p in D.backbone.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    opt_D = torch.optim.AdamW(filter(lambda p: p.requires_grad, D.parameters()), lr=cfg.LR_D)\n",
        "\n",
        "del warmup_history, data_iter, progress\n",
        "try:\n",
        "    del opt_D_warmup\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(f\"\\nMemory cleaned. Free GPU: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")\n",
        "print(\"Ready for Phase 2...\")\n",
        "\n",
        "\n",
        "# --- Phase 2: Main Training Loop ---\n",
        "print(f\"\\nStarting training for {cfg.TRAIN_OPTIM_STEPS_THIS_RUN} steps...\")\n",
        "G.train()\n",
        "D.train()\n",
        "D.backbone.eval()\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "real_history = {'step': [], 'd_loss': [], 'g_diff': [], 'g_adv': [], 'lambda': []}\n",
        "\n",
        "def adv_weight_for_step(step):\n",
        "    if cfg.ADV_RAMP_STEPS <= 0:\n",
        "        return cfg.LAMBDA_ADV\n",
        "    return cfg.LAMBDA_ADV * min(step / cfg.ADV_RAMP_STEPS, 1.0)\n",
        "\n",
        "def get_pred_xstart(x_t, t, y):\n",
        "    if cfg.REC_USE_P_MEAN_VARIANCE_PRED_XSTART:\n",
        "        out = diffusion.p_mean_variance(G, x_t, t, model_kwargs={\"y\": y})\n",
        "        return out[\"pred_xstart\"].clamp(-1, 1)\n",
        "    else:\n",
        "        out = G(x_t, t, y=y)\n",
        "        eps, _ = torch.split(out, 3, dim=1)\n",
        "        return diffusion._predict_xstart_from_eps(x_t, t, eps).clamp(-1, 1)\n",
        "\n",
        "progress = tqdm(total=cfg.TRAIN_OPTIM_STEPS_THIS_RUN, initial=0)\n",
        "data_iter = iter(loader)\n",
        "steps_done = 0\n",
        "\n",
        "while steps_done < cfg.TRAIN_OPTIM_STEPS_THIS_RUN:\n",
        "    try:\n",
        "        real_x, y = next(data_iter)\n",
        "    except StopIteration:\n",
        "        data_iter = iter(loader)\n",
        "        real_x, y = next(data_iter)\n",
        "\n",
        "    real_x, y = real_x.to(cfg.DEVICE), y.to(cfg.DEVICE)\n",
        "    t = torch.randint(0, int(diffusion.num_timesteps), (real_x.shape[0],), device=cfg.DEVICE)\n",
        "    noise = torch.randn_like(real_x)\n",
        "    x_t = diffusion.q_sample(real_x, t, noise=noise)\n",
        "\n",
        "    # Train Discriminator\n",
        "    with torch.no_grad():\n",
        "        fake_x0 = get_pred_xstart(x_t, t, y)\n",
        "\n",
        "    real_logits = D(real_x, y)\n",
        "    fake_logits = D(fake_x0.detach(), y)\n",
        "    d_loss = bce(real_logits, torch.ones_like(real_logits)) + bce(fake_logits, torch.zeros_like(fake_logits))\n",
        "\n",
        "    opt_D.zero_grad()\n",
        "    d_loss.backward()\n",
        "    opt_D.step()\n",
        "\n",
        "    # Train Generator\n",
        "    model_output = G(x_t, t, y=y)\n",
        "    eps_pred, _ = torch.split(model_output, 3, dim=1)\n",
        "    diff_loss = F.mse_loss(eps_pred, noise)\n",
        "\n",
        "    T_max = max(int(diffusion.num_timesteps), 2)\n",
        "    adv_on = (t.float().mean().item() / (T_max - 1)) <= cfg.ADV_GUIDANCE_REGION\n",
        "\n",
        "    loss_adv = torch.tensor(0.0, device=cfg.DEVICE)\n",
        "    if adv_on:\n",
        "        fake_x0_g = get_pred_xstart(x_t, t, y)\n",
        "        fake_logits_g = D(fake_x0_g, y)\n",
        "        loss_adv = bce(fake_logits_g, torch.ones_like(fake_logits_g))\n",
        "\n",
        "    lam = adv_weight_for_step(global_step)\n",
        "    total_g_loss = diff_loss + lam * loss_adv\n",
        "\n",
        "    opt_G.zero_grad()\n",
        "    total_g_loss.backward()\n",
        "    opt_G.step()\n",
        "\n",
        "    real_history['step'].append(global_step)\n",
        "    real_history['d_loss'].append(d_loss.item())\n",
        "    real_history['g_diff'].append(diff_loss.item())\n",
        "    real_history['g_adv'].append(loss_adv.item() if torch.is_tensor(loss_adv) else loss_adv)\n",
        "    real_history['lambda'].append(lam)\n",
        "\n",
        "    global_step += 1\n",
        "    steps_done += 1\n",
        "    progress.update(1)\n",
        "\n",
        "    if steps_done % cfg.LOG_EVERY == 0:\n",
        "        progress.set_description(f\"D={d_loss.item():.3f} G={diff_loss.item():.3f} L={lam:.4f}\")\n",
        "\n",
        "    if global_step % cfg.SAVE_EVERY == 0:\n",
        "        save_checkpoint(global_step, G, D, opt_G, opt_D)\n",
        "\n",
        "    if steps_done % 50 == 0:\n",
        "        aggressive_cleanup()\n",
        "\n",
        "progress.close()\n",
        "save_checkpoint(global_step, G, D, opt_G, opt_D)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "\n",
        "# --- Visual Comparison ---\n",
        "print(\"\\nGenerating comparison images...\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def quick_sample(model, steps=\"50\", n=4):\n",
        "    args = dict(diffusion_args)\n",
        "    args[\"timestep_respacing\"] = steps\n",
        "    _, diff = script_util.create_model_and_diffusion(**args)\n",
        "\n",
        "    y = torch.tensor(cfg.TARGET_CLASSES, device=cfg.DEVICE)\n",
        "    y = y[torch.randint(0, len(y), (n,), device=cfg.DEVICE)]\n",
        "    noise = torch.randn(n, 3, 64, 64, device=cfg.DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "    samples = diff.p_sample_loop(model, (n, 3, 64, 64), noise=noise, model_kwargs={\"y\": y}, progress=True)\n",
        "    return samples.clamp(-1, 1).cpu()\n",
        "\n",
        "base_G, _ = script_util.create_model_and_diffusion(**diffusion_args)\n",
        "base_G.to(cfg.DEVICE).eval()\n",
        "base_G.load_state_dict(_extract_state_dict(torch.load(cfg.PRETRAINED_DIFFUSION_WEIGHTS, map_location=\"cpu\")))\n",
        "\n",
        "print(\"Sampling pretrained model...\")\n",
        "base_imgs = quick_sample(base_G)\n",
        "print(\"Sampling fine-tuned model...\")\n",
        "curr_imgs = quick_sample(G)\n",
        "\n",
        "fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n",
        "for i in range(4):\n",
        "    ax[0, i].imshow(((base_imgs[i] + 1) / 2).permute(1, 2, 0))\n",
        "    ax[0, i].set_title(\"Pretrained\"); ax[0, i].axis(\"off\")\n",
        "    ax[1, i].imshow(((curr_imgs[i] + 1) / 2).permute(1, 2, 0))\n",
        "    ax[1, i].set_title(\"Fine-tuned\"); ax[1, i].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(cfg.EVAL_SAVE_PATH)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6uEUKKanV75m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_path(cfg):\n",
        "    if hasattr(cfg, \"PRETRAINED_DIFFUSION_WEIGHTS\"):\n",
        "        return cfg.PRETRAINED_DIFFUSION_WEIGHTS\n",
        "    return \"./models/64x64_diffusion.pt\"\n",
        "\n",
        "def get_diffusion_defaults(respacing=\"50\"):\n",
        "    defaults = script_util.model_and_diffusion_defaults()\n",
        "    defaults.update({\n",
        "        \"image_size\": 64,\n",
        "        \"num_channels\": 192,\n",
        "        \"num_res_blocks\": 3,\n",
        "        \"learn_sigma\": True,\n",
        "        \"class_cond\": True,\n",
        "        \"use_checkpoint\": True,\n",
        "        \"attention_resolutions\": \"32,16,8\",\n",
        "        \"num_heads\": 4,\n",
        "        \"num_head_channels\": 64,\n",
        "        \"use_scale_shift_norm\": True,\n",
        "        \"dropout\": 0.1,\n",
        "        \"resblock_updown\": True,\n",
        "        \"use_fp16\": False,\n",
        "        \"use_new_attention_order\": True,\n",
        "        \"diffusion_steps\": 1000,\n",
        "        \"timestep_respacing\": respacing,\n",
        "        \"channel_mult\": \"1,2,3,4\"\n",
        "    })\n",
        "    return defaults\n",
        "\n",
        "def build_eval_diffusion(eval_respacing=\"50\"):\n",
        "    _, eval_diffusion = script_util.create_model_and_diffusion(**get_diffusion_defaults(eval_respacing))\n",
        "    return eval_diffusion\n",
        "\n",
        "def build_eval_model_from_pretrained(eval_respacing=\"50\"):\n",
        "    base_G, _ = script_util.create_model_and_diffusion(**get_diffusion_defaults(eval_respacing))\n",
        "    base_G.to(cfg.DEVICE)\n",
        "\n",
        "    w_path = get_weights_path(cfg)\n",
        "    print(f\"Loading pretrained weights from: {w_path}\")\n",
        "\n",
        "    if os.path.exists(w_path):\n",
        "        ckpt = torch.load(w_path, map_location=\"cpu\")\n",
        "        base_G.load_state_dict(_extract_state_dict(ckpt), strict=False)\n",
        "    else:\n",
        "        print(f\"Warning: weights not found at {w_path}. Model is randomly initialized.\")\n",
        "\n",
        "    base_G.eval()\n",
        "    return base_G\n",
        "\n",
        "def sample_images_with_eval_diffusion(model, eval_diffusion, y, noise):\n",
        "    model.eval()\n",
        "    samples = eval_diffusion.p_sample_loop(\n",
        "        model,\n",
        "        shape=noise.shape,\n",
        "        noise=noise,\n",
        "        clip_denoised=True,\n",
        "        model_kwargs={\"y\": y},\n",
        "        progress=True,\n",
        "        device=cfg.DEVICE\n",
        "    )\n",
        "    return samples.clamp(-1, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def smart_empirical_comparison(n_tests=32):\n",
        "    print(f\"Running discriminator evaluation on {n_tests} images...\")\n",
        "\n",
        "    G.eval()\n",
        "    D.eval()\n",
        "\n",
        "    eval_diffusion = build_eval_diffusion(eval_respacing=\"50\")\n",
        "\n",
        "    # Real images\n",
        "    print(\"[1/3] Sampling real images...\")\n",
        "    real_scores = []\n",
        "    collected_labels = []\n",
        "    needed = n_tests\n",
        "    data_iter = iter(loader)\n",
        "\n",
        "    while needed > 0:\n",
        "        try:\n",
        "            x, y = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(loader)\n",
        "            x, y = next(data_iter)\n",
        "\n",
        "        take = min(x.size(0), needed)\n",
        "        x_batch = x[:take].to(cfg.DEVICE)\n",
        "        y_batch = y[:take].to(cfg.DEVICE)\n",
        "\n",
        "        real_scores.append(torch.sigmoid(D(x_batch, y_batch)).mean().item())\n",
        "        collected_labels.append(y_batch)\n",
        "        needed -= take\n",
        "\n",
        "    avg_real = sum(real_scores) / len(real_scores)\n",
        "    labels = torch.cat(collected_labels, dim=0)[:n_tests]\n",
        "    noise = torch.randn(n_tests, 3, cfg.IMAGE_SIZE, cfg.IMAGE_SIZE, device=cfg.DEVICE)\n",
        "\n",
        "    # Pretrained model\n",
        "    print(\"[2/3] Sampling from pretrained model...\")\n",
        "    base_G = build_eval_model_from_pretrained(eval_respacing=\"50\")\n",
        "    base_samples = sample_images_with_eval_diffusion(base_G, eval_diffusion, labels, noise)\n",
        "    avg_base = torch.sigmoid(D(base_samples, labels)).mean().item()\n",
        "\n",
        "    del base_G\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Fine-tuned model\n",
        "    print(\"[3/3] Sampling from fine-tuned model...\")\n",
        "    curr_samples = sample_images_with_eval_diffusion(G, eval_diffusion, labels, noise)\n",
        "    avg_curr = torch.sigmoid(D(curr_samples, labels)).mean().item()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Discriminator scores (0.0 = fake, 1.0 = real):\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"Source\": [\"Real Images\", \"Fine-Tuned Model\", \"Pretrained Model\"],\n",
        "        \"Score\": [avg_real, avg_curr, avg_base]\n",
        "    })\n",
        "    print(df)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if avg_curr > avg_base:\n",
        "        diff = (avg_curr - avg_base) / avg_base * 100\n",
        "        print(f\"Improvement: +{diff:.1f}% realism over pretrained baseline.\")\n",
        "    else:\n",
        "        print(\"No improvement in discriminator score over pretrained baseline.\")\n",
        "\n",
        "smart_empirical_comparison(n_tests=32)"
      ],
      "metadata": {
        "id": "lHo8pWzmbdJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_N = 100\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "eval_diffusion = diffusion\n",
        "G = G.to(cfg.DEVICE).eval()\n",
        "\n",
        "print(f\"Starting final evaluation on {TARGET_N} images...\")\n",
        "\n",
        "# Load base_G by cloning G to guarantee architecture match\n",
        "print(\"[1/4] Loading models...\")\n",
        "\n",
        "def smart_load(model, raw_sd):\n",
        "    model_sd = model.state_dict()\n",
        "    stripped = {}\n",
        "    for k, v in raw_sd.items():\n",
        "        for p in [\"module.\", \"ema.\", \"model.\", \"diffusion_model.\"]:\n",
        "            if k.startswith(p):\n",
        "                k = k[len(p):]\n",
        "        stripped[k] = v\n",
        "\n",
        "    matched = {\n",
        "        k: v for k, v in stripped.items()\n",
        "        if k in model_sd and hasattr(v, \"shape\") and v.shape == model_sd[k].shape\n",
        "    }\n",
        "    missing, unexpected = model.load_state_dict(matched, strict=False)\n",
        "    loaded = len(model_sd) - len(missing)\n",
        "    print(f\"   Loaded: {len(matched)} tensors ({loaded}/{len(model_sd)}), missing: {len(missing)}, unexpected: {len(unexpected)}\")\n",
        "    if loaded < 0.8 * len(model_sd):\n",
        "        print(f\"   Warning: loaded <80% of weights. First missing keys: {list(missing)[:5]}\")\n",
        "\n",
        "base_G = copy.deepcopy(G).to(cfg.DEVICE).eval()\n",
        "w_path = get_weights_path(cfg)\n",
        "print(f\"   Loading pretrained weights from: {w_path}\")\n",
        "\n",
        "if not os.path.exists(w_path):\n",
        "    raise FileNotFoundError(f\"Weights file not found: {w_path}\")\n",
        "\n",
        "ckpt = torch.load(w_path, map_location=\"cpu\")\n",
        "smart_load(base_G, _extract_state_dict(ckpt))\n",
        "\n",
        "# Collect real images & generate samples\n",
        "print(\"[2/4] Generating samples...\")\n",
        "\n",
        "real_imgs = []\n",
        "got = 0\n",
        "data_iter = iter(loader)\n",
        "while got < TARGET_N:\n",
        "    try:\n",
        "        xb, _ = next(data_iter)\n",
        "    except StopIteration:\n",
        "        data_iter = iter(loader)\n",
        "        xb, _ = next(data_iter)\n",
        "    take = min(xb.size(0), TARGET_N - got)\n",
        "    real_imgs.append(xb[:take].cpu())\n",
        "    got += take\n",
        "\n",
        "real_imgs = torch.cat(real_imgs, dim=0)[:TARGET_N]\n",
        "\n",
        "fake_pre, fake_ft = [], []\n",
        "generated = 0\n",
        "pbar = tqdm(total=TARGET_N)\n",
        "targets = torch.tensor(cfg.TARGET_CLASSES, device=cfg.DEVICE)\n",
        "\n",
        "while generated < TARGET_N:\n",
        "    curr_bs = min(BATCH_SIZE, TARGET_N - generated)\n",
        "    labels = targets[torch.randint(0, len(targets), (curr_bs,), device=cfg.DEVICE)]\n",
        "    noise = torch.randn(curr_bs, 3, 64, 64, device=cfg.DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_pre = eval_diffusion.p_sample_loop(\n",
        "            base_G, shape=noise.shape, noise=noise, clip_denoised=True,\n",
        "            model_kwargs={\"y\": labels}, progress=False, device=cfg.DEVICE\n",
        "        ).clamp(-1, 1)\n",
        "        fake_pre.append(ensure_nchw(out_pre))\n",
        "\n",
        "        out_ft = eval_diffusion.p_sample_loop(\n",
        "            G, shape=noise.shape, noise=noise, clip_denoised=True,\n",
        "            model_kwargs={\"y\": labels}, progress=False, device=cfg.DEVICE\n",
        "        ).clamp(-1, 1)\n",
        "        fake_ft.append(ensure_nchw(out_ft))\n",
        "\n",
        "    generated += curr_bs\n",
        "    pbar.update(curr_bs)\n",
        "\n",
        "pbar.close()\n",
        "fake_pretrained = torch.cat(fake_pre, dim=0)\n",
        "fake_finetuned  = torch.cat(fake_ft, dim=0)\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Visual check\n",
        "print(\"\\n[3/4] Visual results...\")\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1); show_grid(real_imgs, \"Real Images\")\n",
        "plt.subplot(1, 3, 2); show_grid(fake_pretrained, \"Pretrained\")\n",
        "plt.subplot(1, 3, 3); show_grid(fake_finetuned, \"Fine-tuned\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Metrics\n",
        "print(\"[4/4] Calculating metrics...\")\n",
        "\n",
        "s_pre = laplacian_variance(to_01(fake_pretrained))\n",
        "s_ft  = laplacian_variance(to_01(fake_finetuned))\n",
        "p_pre, r_pre = compute_pr_metrics(real_imgs, fake_pretrained)\n",
        "p_ft,  r_ft  = compute_pr_metrics(real_imgs, fake_finetuned)\n",
        "\n",
        "def pct_change(new, old, eps=1e-6):\n",
        "    return ((new - old) / (abs(old) + eps)) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"{'Metric':<15} | {'Pretrained':<12} | {'Fine-tuned':<12} | {'Change'}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Sharpness':<15} | {s_pre:.4f}       | {s_ft:.4f}       | {pct_change(s_ft,  s_pre):+.1f}%\")\n",
        "print(f\"{'Precision':<15} | {p_pre:.4f}       | {p_ft:.4f}       | {pct_change(p_ft,  p_pre):+.1f}%\")\n",
        "print(f\"{'Recall':<15}    | {r_pre:.4f}       | {r_ft:.4f}       | {pct_change(r_ft,  r_pre):+.1f}%\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "2kSE9N_jbdr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}